---
title: Agents
description: Explore how agents operate, how they use tools and memory, and how to structure them effectively in Peargent.
---

<h1 className="mt-4 text-3xl font-medium">
  <span className="text-primary">Agents</span>
</h1>
<h3 className="-mt-7 text-lg font-normal text-muted-foreground">
  Explore how agents operate, how they use tools and memory, and how to structure them effectively in Peargent.
</h3>
In simple terms, an Agent calls the **<u>[Model](/docs/models)</u>** to generate responses according to its defined behavior.

Agents are the core units of work in Peargent, while **<u>[Tools](/docs/tools)</u>** provide the capabilities that help agents perform actions and tackle complex tasks.

Agents can operate individually for simple tasks, or they can be combined into a **<u>[Pool](/docs/pools)</u>** of agents to handle more complex, multi-step workflows.

## Creating an Agent

To create an agent, use the `create_agent` function from the peargent module. At minimum, you must define the agent’s `name`, `description`, and `persona`, and the `model` to use.

Here is a simple example:

```python
from peargent import create_agent
from peargent.models import openai

code_reviewer = create_agent(
    name="Code Reviewer",
    description="Reviews code for issues and improvements",
    persona=(
        "You are a highly skilled senior software engineer and code reviewer. "
        "Your job is to analyze code for correctness, readability, maintainability, and performance. "
        "Identify bugs, edge cases, and bad practices. Suggest improvements that follow modern Python "
        "standards and best engineering principles. Provide clear explanations and, when appropriate, "
        "offer improved code snippets. Always be concise, accurate, and constructive."
    ),
    model=openai("gpt-4")
)
```
Call `agent.run(prompt)` to perform an inference using the agent’s persona as the system prompt and your input as the user message.

```python
response = code_reviewer.run("Review this Python function for improvements:\n\ndef add(a, b): return a+b")
print(response)
# The function is correct but could be optimized, here is the optimized version...
```
<Callout >
  When running an agent individually, the `description` field is optional. However, it becomes mandatory when the agent is part of a **<u>[Pool](/docs/pools)</u>**.
</Callout>

* Refer **<u>[Tools](/docs/tools)</u>** to learn how to use tools with agents.
* Refer **<u>[History](/docs/history)</u>** to learn how to setup conversation memory for agents.
* Refer **<u>[Pool](/docs/pools)</u>** to learn how to create a pool of agents.


## How does agent work?

<div className='fd-steps [&_h3]:fd-step'>

### Start Execution `(agent.run())`

When you call `agent.run(...)`, the agent prepares for a new interaction: it loads any previous conversation **<u>[History](/docs/history)</u>** (if enabled), begins **<u>tracing</u>** (if enabled), and registers the user's new input.

### Build the Prompt

The agent constructs the full prompt by combining its **<u>persona</u>**, **<u>[Tools](/docs/tools)</u>**, prior conversation context, and optional **<u>output schema</u>**. This prompt is then sent to the configured **<u>[Model](/docs/models)</u>**.

### Model Generates a Response

The model returns a response based on the prompt. The agent records this output and checks whether the model is requesting **<u>tool calls</u>**.

### Execute Tools (If Requested)

If the response includes tool calls, the agent runs those tools (in **<u>parallel</u>** if multiple), collects their outputs, and then asks the model again using an updated prompt. This cycle continues until no more tool actions are required.

### Finalize the Result

The agent checks whether it should stop (stop conditions met or max iterations reached). If an **<u>output schema</u>** was provided, the response is validated against it. Finally, the conversation is synced to **<u>[History](/docs/history)</u>** (if enabled), tracing is ended, and the final response is returned.

</div>

## Parameters

| Parameter | Type | Description | Required |
| :--- | :--- | :--- | :--- |
| `name` | `str` | The name of the agent. | Yes |
| `description` | `str` | A brief description of the agent's purpose. Required when using in a **<u>[Pool](/docs/pools)</u>**. | No* |
| `persona` | `str` | The system prompt defining the agent's personality and instructions. | Yes |
| `model` | `Model` | The LLM model instance (e.g., `openai("gpt-4")`). | Yes |
| `tools` | `list[Tool]` | A list of **<u>[Tools](/docs/tools)</u>** the agent can access. | No |
| `stop` | `StopCondition` | Condition that determines when the agent should stop iterating (default: `limit_steps(5)`). | No |
| `history` | `HistoryConfig` | Configuration for conversation **<u>[History](/docs/history)</u>**. | No |
| `tracing` | `bool \| None` | Enable/disable tracing. `None` (default) inherits from global tracer if `enable_tracing()` was called, `True` explicitly enables, `False` opts out. | No |
| `output_schema` | `Type[BaseModel]` | Pydantic model for structured output validation. | No |
| `max_retries` | `int` | Maximum retries for `output_schema` validation (default: `3`). Only used when `output_schema` is provided. | No |

\* Required when using the agent in a **<u>[Pool](/docs/pools)</u>**.

To know more about `stop`, `tracing`, `output_schema`, and `max_retries`, refer to **<u>Advanced Features</u>**.